from __future__ import annotations
from typing import Optional, List, Tuple, Sequence, Dict

import os
import re
import html
import argparse

import psycopg
from dotenv import load_dotenv

# Import from new utils modules
from utils.db import _pg, _table_exists, _doc_tuple_to_meta
from utils.ai import _embed, _chat, _to_pgvector
from utils.text import strip_tags, normalize, tokenize, keyword_terms

# -----------------------------
# Env
# -----------------------------
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
DATABASE_URL   = os.getenv("DATABASE_URL")

# -----------------------------
# Project hints for disambiguation
# -----------------------------
PROJECT_HINTS = {
    "estate 360": "Estate 360",
    "the sanctuaries": "The Sanctuaries",
    "sanctuaries": "The Sanctuaries",
    "trevoc": "Trevoc 56",
    "trevoc 56": "Trevoc 56",
    "godrej sora": "Godrej Sora",
    "project 1": "Project 1",
    "tarc ishva": "Tarc Ishva",
    "ishva": "Tarc Ishva",
    "anant raj": "The Estate Residences",
}

# ----------------------------------------
# Helper: Derive intent tag from question
# ----------------------------------------
def intent_tag(question: str) -> Optional[str]:
    """
    Detect query intent from question text.
    Returns intent tag: "payment", "amenities", "location", or None.
    """
    ql = question.lower()

    # Payment-related queries (expanded aliases)
    payment_indicators = [
        "payment plan", "payment schedule", "payment structure",
        "construction linked", "possession linked", "clp", "plp",
        "price list", "eoi", "allotment", "ats", "registry",
        "down payment", "downpayment", "booking amount", "token amount",
        "emi", "installment", "milestone", "payment milestone",
        "subvention", "flexi payment", "flexible payment",
        "assured returns", "rental guarantee"
    ]
    if any(p in ql for p in payment_indicators):
        return "payment"

    # Amenities queries
    if any(p in ql for p in ["amenity", "amenities", "club", "wellness", "gym", "pool", "spa", "facilities"]):
        return "amenities"

    # Location queries
    if any(p in ql for p in ["location", "sector", "address", "nearby", "connectivity", "distance"]):
        return "location"

    return None

# -----------------------------
# OpenAI helpers
# -----------------------------
def _embed(texts: List[str]) -> List[List[float]]:
    from openai import OpenAI
    client = OpenAI(api_key=OPENAI_API_KEY, base_url=os.getenv("OPENAI_BASE_URL"))
    resp = client.embeddings.create(model="text-embedding-3-small", input=texts)
    return [d.embedding for d in resp.data]

def _chat(prompt: str, model: str = "gpt-4.1-mini") -> str:
    from openai import OpenAI
    client = OpenAI(api_key=OPENAI_API_KEY, base_url=os.getenv("OPENAI_BASE_URL"))
    resp = client.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": "Be precise. Use only provided context."},
            {"role": "user", "content": prompt},
        ],
        temperature=0,
    )
    return resp.choices[0].message.content

def _to_pgvector(vec: List[float]) -> str:
    return "[" + ",".join(f"{x:.8f}" for x in vec) + "]"

def _pg() -> psycopg.Connection:
    if not DATABASE_URL:
        raise SystemExit("DATABASE_URL not set in InvestoChat_Build/.env")
    return psycopg.connect(DATABASE_URL)

# -----------------------------
# Token + MMR utils
# -----------------------------
DOMAIN_TERMS = {
    "bhk","sq","sft","sqft","acre","acres","tower","core","aravalli","gurgaon","gurugram","sector",
    "clp","plp","possession","rera","super","area","carpet","price","launch","amenities","lakh",
    "crore","flexi","payment","green","club","wellness","noida"
}
SAFE_CHARS = r"[^a-z0-9 ₹.%/-]+"

def tokenize(text: str):
    t = re.sub(SAFE_CHARS, " ", text.lower())
    raw = [w for w in t.split() if w]
    stop = {"the","a","an","and","or","of","to","in","on","for","with","at","by","from",
            "is","are","was","were","be","as","that","this","these","those"}
    keep = []
    for w in raw:
        if w in stop:
            continue
        if w in DOMAIN_TERMS:
            keep.append(w); continue
        if w.replace(".","",1).isdigit():
            keep.append(w); continue
        if any(s in w for s in ["bhk","sq","ft","sft","₹","cr","lakh","%"]):
            keep.append(w); continue
        keep.append(w)
    if os.getenv("DEBUG_RAG") == "1":
        print("[tokens]", keep[:40])
    return keep


# ----------------------------------------
# Helper: Extract robust search terms from question
# ----------------------------------------
def keyword_terms(question: str) -> List[str]:
    ql = question.lower()
    # High-priority domain phrases
    payment_aliases = [
        "payment plan", "payment schedule", "construction linked",
        "possession linked", "clp", "plp", "price list", "eoi",
        "allotment", "ats", "registry"
    ]
    terms: List[str] = []
    if any(p in ql for p in payment_aliases):
        # Keep canonical phrases and any alias actually present in the question
        terms = ["payment plan"] + [p for p in payment_aliases if p in ql and p != "payment plan"]
    else:
        # Fall back to content tokens from the question
        toks = tokenize(question)
        # Keep up to 5 nontrivial tokens to avoid over-broad scans
        terms = [t for t in toks if len(t) > 2][:5]
        if not terms:
            terms = [ql.strip()[:24]]  # final fallback
    # Deduplicate while preserving order
    seen = set()
    out = []
    for t in terms:
        if t and t not in seen:
            out.append(t)
            seen.add(t)
    return out


# ----------------------------------------
# Helper: Detect normalized project name for filtering
# ----------------------------------------
def detect_project_filter(question: str, explicit: Optional[str] = None) -> Optional[str]:
    """
    Return a normalized project name to filter the `project` column
    (e.g., 'The Sanctuaries'). If `explicit` is provided, use that.
    """
    if explicit:
        return explicit
    ql = question.lower()
    for k, v in PROJECT_HINTS.items():
        if k in ql:
            return v
    return None

def get_project_id_from_name(project_name: str) -> Optional[int]:
    """
    Convert project name to project_id by querying the database.
    Returns None if project not found.
    """
    if not project_name:
        return None

    try:
        with _pg() as con, con.cursor() as cur:
            # Try exact match first
            cur.execute("SELECT id FROM projects WHERE name = %s", (project_name,))
            result = cur.fetchone()
            if result:
                return result[0]

            # Try case-insensitive match
            cur.execute("SELECT id FROM projects WHERE LOWER(name) = LOWER(%s)", (project_name,))
            result = cur.fetchone()
            if result:
                return result[0]

            # Try slug match
            slug = project_name.lower().replace(' ', '-')
            cur.execute("SELECT id FROM projects WHERE slug = %s", (slug,))
            result = cur.fetchone()
            if result:
                return result[0]

    except Exception as e:
        print(f"[warn] Failed to get project_id for '{project_name}': {e}")

    return None

def _sim_token_overlap(si: set, sj: set) -> float:
    inter = len(si & sj)
    return inter / max(1, min(len(si), len(sj)))

def _has_payment_table(text: str) -> bool:
    """
    Detect if text contains a structured payment schedule table.
    Looks for table indicators with payment-related headers.
    """
    text_lower = text.lower()
    lines = text_lower.split('\n')

    # Look for lines with table structure (2+ pipe characters)
    table_lines = [ln for ln in lines if ln.count('|') >= 2]
    if not table_lines:
        return False

    # Check if any table line contains payment indicators
    payment_indicators = [
        "milestone", "payment", "installment", "stage", "%",
        "amount", "due", "receipt", "booking", "possession",
        "construction", "registry", "handover", "clp", "plp"
    ]

    # Check first few table lines (likely to be headers)
    for line in table_lines[:3]:
        if any(indicator in line for indicator in payment_indicators):
            return True

    return False

def score(doc: str, meta: dict, qtokens, intent: Optional[str] = None) -> float:
    """
    Score a document chunk for relevance to the query.

    Args:
        doc: Document text
        meta: Metadata dictionary
        qtokens: Query tokens
        intent: Intent tag (e.g., "payment", "amenities", "location")

    Returns:
        Relevance score (higher is better)
    """
    dl = doc.lower()
    overlap = sum(1 for t in qtokens if f" {t} " in f" {dl} ")

    # Base metadata boost
    boost = 0.0
    for key in ("source", "project", "section", "doc_id"):
        v = str(meta.get(key, "")).lower()
        if v:
            boost += 1.5 * sum(1 for t in qtokens if t in v)

    # Intent-specific boosts
    if intent == "payment":
        # Boost chunks with table structure containing payment info
        if _has_payment_table(doc):
            boost += 8.0  # Strong boost for payment tables

        # Boost chunks with percentage markers (payment milestones)
        percent_count = doc.count('%')
        boost += min(percent_count * 1.5, 6.0)  # Cap at 6.0

        # Boost specific page ranges (payment plans often on p.10-25)
        page = meta.get("page", 0)
        if isinstance(page, int) and 10 <= page <= 25:
            boost += 2.0

        # Boost chunks with payment-related terms
        payment_terms = ["clp", "plp", "milestone", "installment", "booking", "possession"]
        boost += 1.5 * sum(1 for term in payment_terms if term in dl)

    elif intent == "amenities":
        # Boost chunks with list structure (amenities often listed)
        bullet_count = doc.count('-')
        boost += min(bullet_count * 0.3, 4.0)  # Cap at 4.0

        # Boost chunks with amenity keywords
        amenity_terms = ["club", "wellness", "gym", "pool", "spa", "garden", "clubhouse", "fitness"]
        boost += 2.0 * sum(1 for term in amenity_terms if term in dl)

    elif intent == "location":
        # Boost chunks with location indicators
        location_terms = ["sector", "road", "metro", "highway", "airport", "distance", "proximity"]
        boost += 2.0 * sum(1 for term in location_terms if term in dl)

    # Length normalization
    length = max(50, len(dl.split()))
    length_norm = min(1.0, 600 / length)

    return (overlap + boost) * length_norm

def mmr(documents, metadatas, qtokens, lambda_=0.75, topk=3, intent: Optional[str] = None):
    """
    Maximal Marginal Relevance for diverse chunk selection.

    Args:
        documents: List of document texts
        metadatas: List of metadata dicts
        qtokens: Query tokens
        lambda_: Balance between relevance (higher) and diversity (lower)
        topk: Number of documents to select
        intent: Intent tag for intent-aware scoring

    Returns:
        Tuple of (selected_documents, selected_metadatas)
    """
    cand = list(range(len(documents)))
    selected = []
    token_sets = [set(d.lower().split()) for d in documents]
    while cand and len(selected) < topk:
        best, best_s = None, -1e9
        for i in cand:
            rel = score(documents[i], metadatas[i], qtokens, intent=intent)
            div = 0 if not selected else max(_sim_token_overlap(token_sets[i], token_sets[j]) for j in selected)
            s = lambda_ * rel - (1 - lambda_) * div
            if s > best_s:
                best, best_s = i, s
        selected.append(best)
        cand.remove(best)
    return [documents[i] for i in selected], [metadatas[i] for i in selected]

def _table_exists(cur, name: str) -> bool:
    cur.execute("SELECT to_regclass(%s)", (name,))
    return cur.fetchone()[0] is not None

def retrieve_sql_ilike(
    question: str,
    k: int = 3,
    overfetch: int = 24,
    project_like: Optional[str] = None,
    tag: Optional[str] = None
):
    terms = keyword_terms(question)
    with _pg() as con, con.cursor() as cur:
        if not _table_exists(cur, "ocr_pages"):
            return {"mode":"empty", "answers":[], "metas":[]}
        where_parts = []
        params: List[object] = []
        # (text ILIKE %term1% OR text ILIKE %term2% ...)
        if terms:
            where_parts.append("(" + " OR ".join(["text ILIKE %s"] * len(terms)) + ")")
            params.extend([f"%{t}%" for t in terms])
        if project_like:
            where_parts.append("project ILIKE %s")
            params.append(f"%{project_like}%")
        if tag:
            where_parts.append("tags = %s")
            params.append(tag)
        where_clause = "WHERE " + " AND ".join(where_parts) if where_parts else ""
        sql = [
            "SELECT source_pdf, page,",
            "       lag(text)  OVER (PARTITION BY source_pdf ORDER BY page) AS prev_text,",
            "       text AS cur_text,",
            "       lead(text) OVER (PARTITION BY source_pdf ORDER BY page) AS next_text",
            "FROM ocr_pages",
            where_clause,
            "ORDER BY (CASE WHEN %s::TEXT IS NOT NULL AND tags = %s::TEXT THEN 0 ELSE 1 END), length(text) ASC",
            "LIMIT %s",
        ]
        params.extend([tag, tag])
        params.append(overfetch)
        cur.execute("\n".join([s for s in sql if s]), tuple(params))
        rows = cur.fetchall()
    docs = []
    metas = []
    for r in rows:
        source_pdf, page, prev_text, cur_text, next_text = r
        combined = " ".join(t for t in [prev_text, cur_text, next_text] if t)
        metas.append({"source": source_pdf, "page": page, "score": 0.0})
        docs.append(combined)
    if not docs:
        return {"mode":"empty", "answers":[], "metas":[]}
    qtokens = tokenize(question)
    top_docs, top_metas = mmr(docs, metas, qtokens, lambda_=0.75, topk=max(1, k), intent=tag)
    return {"mode":"ocr_ilike", "answers": top_docs, "metas": top_metas}

def retrieve_sql_trgm(
    question: str,
    k: int = 3,
    overfetch: int = 24,
    project_like: Optional[str] = None,
    tag: Optional[str] = None
):
    terms = keyword_terms(question)
    with _pg() as con, con.cursor() as cur:
        if not _table_exists(cur, "ocr_pages"):
            return {"mode":"empty", "answers":[], "metas":[]}
        # Build GREATEST(similarity(text, t1), similarity(text, t2), ...)
        if not terms:
            terms = [question]
        sim_expr = "GREATEST(" + ",".join(["similarity(text, %s)"] * len(terms)) + ")"
        params: List[object] = terms.copy()
        where_parts = []
        if project_like:
            where_parts.append("project ILIKE %s")
            params.append(f"%{project_like}%")
        if tag:
            where_parts.append("tags = %s")
            params.append(tag)
        where_clause = "WHERE " + " AND ".join(where_parts) if where_parts else ""
        sql = [
            "SELECT source_pdf, page,",
            "       lag(text)  OVER (PARTITION BY source_pdf ORDER BY page) AS prev_text,",
            "       text AS cur_text,",
            "       lead(text) OVER (PARTITION BY source_pdf ORDER BY page) AS next_text,",
            f"       {sim_expr} AS s",
            "FROM ocr_pages",
            where_clause,
            "ORDER BY (CASE WHEN %s::TEXT IS NOT NULL AND tags = %s::TEXT THEN 0 ELSE 1 END), s DESC",
            "LIMIT %s",
        ]
        params.extend([tag, tag])
        params.append(overfetch)
        cur.execute("\n".join([s for s in sql if s]), tuple(params))
        rows = cur.fetchall()
    docs = []
    metas = []
    for r in rows:
        source_pdf, page, prev_text, cur_text, next_text, s = r
        combined = " ".join(t for t in [prev_text, cur_text, next_text] if t)
        docs.append(combined)
        metas.append({"source": source_pdf, "page": page, "score": float(s)})
    if not docs:
        return {"mode":"empty", "answers":[], "metas":[]}
    qtokens = tokenize(question)
    top_docs, top_metas = mmr(docs, metas, qtokens, lambda_=0.75, topk=max(1, k), intent=tag)
    return {"mode":"ocr_trgm", "answers": top_docs, "metas": top_metas}

# -----------------------------
# Postgres retrieval
# -----------------------------
def _doc_tuple_to_meta(row) -> dict:
    # row order: text, page, section, source_path, score
    return {"page": row[1], "section": row[2], "source": row[3], "score": float(row[4])}

def search_facts(qvec: List[float], k: int = 8, project_id: Optional[int] = None) -> List[Tuple[str, dict]]:
    with _pg() as con, con.cursor() as cur:
        if not _table_exists(cur, "facts"):
            return []
        qvec_str = _to_pgvector(qvec)
        where_clauses = ["embedding IS NOT NULL"]
        if project_id is not None:
            where_clauses.insert(0, "project_id = %s")
            # Placeholders: $1=qvec(SELECT), $2=project_id(WHERE), $3=qvec(ORDER), $4=k(LIMIT)
            params = [qvec_str, project_id, qvec_str, k]
        else:
            # Placeholders: $1=qvec(SELECT), $2=qvec(ORDER), $3=k(LIMIT)
            params = [qvec_str, qvec_str, k]
        cur.execute(
            """
            SELECT value, source_page, key, 1 - (embedding <=> %s::vector) AS score
            FROM facts
            WHERE """ + " AND ".join(where_clauses) + """
            ORDER BY embedding <=> %s::vector
            LIMIT %s
            """,
            tuple(params),
        )
        rows = cur.fetchall()
    out = []
    for value, page, key, score in rows:
        out.append((value, {"page": page, "key": key, "score": float(score)}))
    return out

def search_docs(qvec: List[float], k: int = 12, project_id: Optional[int] = None) -> List[Tuple[str, dict]]:
    with _pg() as con, con.cursor() as cur:
        if not _table_exists(cur, "documents"):
            return []
        qvec_str = _to_pgvector(qvec)
        if project_id is not None:
            where_clause = "WHERE project_id = %s"
            # Placeholders: $1=qvec(SELECT), $2=project_id(WHERE), $3=qvec(ORDER), $4=k(LIMIT)
            params = [qvec_str, project_id, qvec_str, k]
        else:
            where_clause = ""
            # Placeholders: $1=qvec(SELECT), $2=qvec(ORDER), $3=k(LIMIT)
            params = [qvec_str, qvec_str, k]
        query = """
            SELECT text, page, section, source_path, 1 - (embedding <=> %s::vector) AS score
            FROM documents
            """ + where_clause + """
            ORDER BY embedding <=> %s::vector
            LIMIT %s
            """
        cur.execute(query, tuple(params))
        rows = cur.fetchall()
    return [(r[0], _doc_tuple_to_meta(r)) for r in rows]

def retrieve(q: str, k: int = 3, overfetch: int = 48, project_id: Optional[int] = None, project_name: Optional[str] = None):
    project_filter = detect_project_filter(q, project_name)
    tag = intent_tag(q)

    # Convert project name to ID if provided but no ID given
    if project_id is None and project_filter:
        project_id = get_project_id_from_name(project_filter)
        if project_id:
            print(f"[project] Filtering by '{project_filter}' (ID: {project_id})")

    if any(kw in q.lower() for kw in ("payment plan","payment schedule","possession linked","construction linked","clp","plp")):
        overfetch = max(overfetch, 96)
        k = max(k, 5)
    use_ocr_first = os.getenv("USE_OCR_SQL") == "1"
    if not use_ocr_first:
        try:
            qvec = _embed([q])[0]
            facts = search_facts(qvec, k=8, project_id=project_id)
            if facts and facts[0][1].get("score", 0) >= 0.5:
                return {"mode":"facts", "answers":[facts[0][0]], "metas":[facts[0][1]]}
            docs = search_docs(qvec, k=overfetch, project_id=project_id)
            if docs:
                documents, metadatas = zip(*docs)
                qtokens = tokenize(q)
                top_docs, top_metas = mmr(list(documents), list(metadatas), qtokens, lambda_=0.75, topk=max(1,k), intent=tag)
                return {"mode":"docs", "answers":top_docs, "metas":top_metas}
        except Exception as e:
            # fall through to OCR SQL if embeddings/vector path fails
            import traceback
            print(f"[DEBUG] Vector search failed: {e}")
            traceback.print_exc()
            pass
    r = retrieve_sql_trgm(q, k=k, overfetch=overfetch, project_like=project_filter, tag=tag)
    if r["mode"] != "empty" and r["answers"]:
        return r
    r = retrieve_sql_ilike(q, k=k, overfetch=overfetch, project_like=project_filter, tag=tag)
    return r

def strip_tags(s: str) -> str:
    if not s:
        return ""
    # unescape HTML entities then remove tags
    s = html.unescape(s)
    s = re.sub(r"<[^>]+>", " ", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s

def normalize(ctx: str) -> str:
    """
    Enhanced text normalization for RAG retrieval context.
    Handles smart quotes, currency symbols, units, and formatting.
    """
    ctx = strip_tags(ctx)

    # Smart quotes normalization
    ctx = ctx.replace('"', '"').replace('"', '"')
    ctx = ctx.replace(''', "'").replace(''', "'")
    ctx = ctx.replace('„', '"').replace('‟', '"')

    # Bullet and dash normalization
    ctx = ctx.replace("•", "- ")
    ctx = ctx.replace("–", "-").replace("—", "-")
    ctx = ctx.replace("●", "- ").replace("○", "- ")

    # Currency normalization (Rs./Rs/INR → ₹, remove spaces after ₹)
    ctx = re.sub(r'\bRs\.?\s*', '₹', ctx, flags=re.IGNORECASE)
    ctx = re.sub(r'\bINR\s*', '₹', ctx, flags=re.IGNORECASE)
    ctx = re.sub(r'₹\s+', '₹', ctx)  # Remove spaces after ₹

    # Unit normalization (preserve common real estate units)
    # sq.ft., sqft, sft, sq. ft. → sq.ft.
    ctx = re.sub(r'\b(sq\.?\s*ft\.?|sqft|sft)\b', 'sq.ft.', ctx, flags=re.IGNORECASE)
    # sq.m., sqm → sq.m.
    ctx = re.sub(r'\b(sq\.?\s*m\.?|sqm)\b', 'sq.m.', ctx, flags=re.IGNORECASE)
    # acres normalization
    ctx = re.sub(r'\bacres?\b', 'acres', ctx, flags=re.IGNORECASE)

    # BHK normalization (bhk, b.h.k, B.H.K → BHK)
    ctx = re.sub(r'\bb\.?\s*h\.?\s*k\.?\b', 'BHK', ctx, flags=re.IGNORECASE)

    # Whitespace normalization
    ctx = re.sub(r"[ \t]+", " ", ctx)
    ctx = re.sub(r"\r?\n[ \t]+", "\n", ctx)
    ctx = re.sub(r"\n{3,}", "\n\n", ctx)

    return ctx.strip()

def answer_from_retrieval(q: str, retrieval: dict, model: str = "gpt-4.1-mini") -> dict:
    mode = retrieval.get("mode", "empty")
    answers = retrieval.get("answers") or []
    metas = retrieval.get("metas") or []
    if mode == "facts" and answers:
        primary = metas[0] if metas else {}
        source = f"{primary.get('source','Brochure')} p.{primary.get('page','?')}"
        return {"answer": answers[0], "mode": "facts", "sources": [{"source": source, **primary}]}
    if not answers:
        return {"answer": "Not in the documents.", "mode": mode, "sources": metas}
    ctx = normalize("\n\n".join(answers))
    if not ctx.strip():
        return {"answer": "Not in the documents.", "mode": mode, "sources": metas}
    source_hint = ""
    if metas:
        # try to infer a single project name from sources
        srcs = {os.path.basename(m.get("source","")) for m in metas if m.get("source")}
        if len(srcs) == 1:
            source_hint = f" Focus ONLY on {list(srcs)[0]}."
    prompt = (
        "You are a retrieval summarizer. Use only the facts in <context>. "
        "If only partial details are present, return the partial payment steps you can verify and say 'partial'. "
        "If nothing relevant is present, reply exactly: 'Not in the documents.'"
        f"{source_hint}\n"
        f"<context>\n{ctx}\n</context>\n"
        f"Question: {q}\nAnswer:"
    )
    if not OPENAI_API_KEY:
        return {"answer": "Not in the documents.", "mode": mode, "sources": metas}
    reply = _chat(prompt, model=model) or "Not in the documents."
    return {"answer": reply, "mode": mode, "sources": metas}

def show(q: str, k: int = 3, project_id: Optional[int] = None, project_name: Optional[str] = None):
    r = retrieve(q, k, project_id=project_id, project_name=project_name)
    if r["mode"] == "facts":
        m = r["metas"][0]
        print(f"[facts] score={m['score']:.3f} page={m.get('page')}: {r['answers'][0]}")
        return
    for i, (doc, meta) in enumerate(zip(r["answers"], r["metas"]), 1):
        page = meta.get("page","?")
        print(f"{i}) p.{page} score={meta.get('score',0):.3f}\n{doc[:600]}\n")

def rag(q: str, k: int = 3, project_id: Optional[int] = None, project_name: Optional[str] = None, model: str = None) -> dict:
    """
    Full RAG pipeline:
      1) retrieve() using vector path (facts/docs) or OCR SQL fallback per USE_OCR_SQL.
      2) summarize with answer_from_retrieval() using chat model.
    Returns a dict: {"answer": str, "mode": str, "sources": list[dict]}.
    """
    if model is None:
        model = os.getenv("CHAT_MODEL", "gpt-4.1-mini")
    r = retrieve(q, k=k, project_id=project_id, project_name=project_name)
    return answer_from_retrieval(q, r, model=model)

# -----------------------------
# CLI
# -----------------------------
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="InvestoChat Postgres RAG tester")
    parser.add_argument("--show", type=str, help="Query to show top-k retrieved chunks")
    parser.add_argument("-k", type=int, default=3, help="Top-k results")
    parser.add_argument("--rag", type=str, help="Query to run full RAG (facts-first + docs)")
    parser.add_argument("--project-id", type=int, help="Limit search to a specific project id")
    parser.add_argument("--project", type=str, help="Project name hint (e.g., 'Sanctuaries', 'Estate 360')")
    args = parser.parse_args()

    # DB health
    try:
        with _pg() as con, con.cursor() as cur:
            docs_n = 0
            ocr_n = 0
            # documents table is optional; only count if present
            if _table_exists(cur, "documents"):
                cur.execute("SELECT count(*) FROM documents")
                docs_n = cur.fetchone()[0]
            else:
                print("[db] documents table missing; skipping vector search path.")
            # ocr_pages is the fallback path used by SQL ILIKE/pg_trgm
            if _table_exists(cur, "ocr_pages"):
                cur.execute("SELECT count(*) FROM ocr_pages")
                ocr_n = cur.fetchone()[0]
            print(f"[db] counts -> documents:{docs_n} ocr_pages:{ocr_n}")
            if os.getenv("USE_OCR_SQL") == "1":
                print("[mode] OCR-SQL first")
            else:
                print("[mode] Vector first (facts/docs)")
    except Exception as e:
        raise SystemExit(f"[db-error] {e}")

    if args.show:
        show(args.show, args.k, project_id=args.project_id, project_name=args.project)
    if args.rag:
        ans = rag(args.rag, args.k, project_id=args.project_id, project_name=args.project)
        print(ans.get("answer", ""))
        srcs = ans.get("sources") or []
        if srcs:
            print("\n[sources]")
            for s in srcs:
                src = s.get("source") or os.path.basename(str(s.get("source", "")))
                page = s.get("page", "?")
                score = s.get("score", 0.0)
                print(f"- {src} p.{page} score={score:.3f}")
